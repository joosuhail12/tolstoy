---
name: Data Pipeline ETL
description: Extract data from API, transform it, and load into database
version: 1
category: Data Processing
author: Tolstoy Team
tags: [etl, data, pipeline, database, api]
inputs:
  - name: source_api_url
    type: string
    required: true
    description: Source API URL for data extraction
  - name: database_connection
    type: string
    required: true
    description: Database connection string
  - name: target_table
    type: string
    required: true
    description: Target table name
---
schedule:
  type: cron
  expression: "0 2 * * *" # Daily at 2 AM
  timezone: "UTC"

steps:
  - key: extract_data
    action: http.get
    inputs:
      url: "{{inputs.source_api_url}}"
      headers:
        Accept: application/json
        Authorization: "Bearer {{secrets.API_TOKEN}}"
      timeout: 30000
    retry:
      attempts: 3
      delay: 5000

  - key: validate_data
    action: validate
    inputs:
      data: "{{extract_data.outputs.body}}"
      schema:
        type: array
        minItems: 1
        items:
          type: object
          required: [id, created_at]

  - key: transform_data
    action: transform
    inputs:
      data: "{{extract_data.outputs.body}}"
      transform: |
        data.map(record => ({
          id: record.id,
          created_date: new Date(record.created_at).toISOString().split('T')[0],
          processed_at: new Date().toISOString(),
          status: record.status || 'active',
          metadata: JSON.stringify(record.metadata || {})
        }))

  - key: load_to_database
    action: database.bulk_insert
    inputs:
      connection: "{{inputs.database_connection}}"
      table: "{{inputs.target_table}}"
      data: "{{transform_data.outputs}}"
      on_conflict: "update"
      batch_size: 1000

  - key: log_metrics
    action: log
    inputs:
      level: info
      message: "ETL pipeline completed successfully"
      metadata:
        records_processed: "{{extract_data.outputs.body.length}}"
        execution_time_ms: "{{execution.duration}}"
        target_table: "{{inputs.target_table}}"
        timestamp: "{{execution.timestamp}}"

  - key: send_success_notification
    action: http.post
    inputs:
      url: "{{secrets.WEBHOOK_URL}}"
      body:
        status: "success"
        pipeline: "data-pipeline-etl"
        records_processed: "{{extract_data.outputs.body.length}}"
        execution_time: "{{execution.duration}}"
    condition:
      field: "{{secrets.WEBHOOK_URL}}"
      operator: "isNotEmpty"