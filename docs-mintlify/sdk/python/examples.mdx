---
title: "Python SDK Examples"
description: "Real-world examples and code recipes using the Tolstoy Python SDK"
---

# Python SDK Examples

This guide provides comprehensive examples and recipes for using the Tolstoy Python SDK in real-world scenarios.

## Basic Operations

### Client Setup and Configuration

<Tabs>
  <Tab title="Environment Variables">
    ```python
    import os
    from tolstoy import TolstoyClient

    # Using environment variables (recommended)
    client = TolstoyClient()  # Automatically loads from environment

    # Or explicitly
    client = TolstoyClient(
        api_key=os.getenv("TOLSTOY_API_KEY"),
        organization_id=os.getenv("TOLSTOY_ORGANIZATION_ID")
    )
    ```
  </Tab>
  
  <Tab title="Configuration File">
    ```python
    import yaml
    from tolstoy import TolstoyClient

    # Load from YAML configuration
    with open("tolstoy-config.yaml", "r") as f:
        config = yaml.safe_load(f)

    client = TolstoyClient(
        api_key=config["api_key"],
        organization_id=config["organization_id"],
        timeout=config.get("timeout", 30),
        max_retries=config.get("max_retries", 3)
    )
    ```
  </Tab>
  
  <Tab title="Advanced Configuration">
    ```python
    import aiohttp
    from tolstoy import TolstoyClient

    # Custom session with connection pooling
    connector = aiohttp.TCPConnector(
        limit=100,
        limit_per_host=20,
        keepalive_timeout=60,
        enable_cleanup_closed=True
    )

    timeout = aiohttp.ClientTimeout(total=30, connect=10)

    async with aiohttp.ClientSession(
        connector=connector, 
        timeout=timeout
    ) as session:
        client = TolstoyClient(
            api_key="your-key",
            organization_id="your-org",
            session=session,
            debug=True
        )
        
        # Use client with custom session
        result = await client.tools.list()
    ```
  </Tab>
</Tabs>

### Error Handling Patterns

```python
import asyncio
import logging
from tolstoy import TolstoyClient
from tolstoy.exceptions import (
    TolstoyError, 
    AuthenticationError, 
    RateLimitError,
    NotFoundError,
    ValidationError
)

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class TolstoyService:
    def __init__(self, api_key: str, organization_id: str):
        self.client = TolstoyClient(
            api_key=api_key,
            organization_id=organization_id,
            max_retries=3
        )
    
    async def safe_execute_action(self, action_id: str, input_data: dict):
        """Execute action with comprehensive error handling"""
        max_attempts = 3
        attempt = 0
        
        while attempt < max_attempts:
            try:
                result = await self.client.actions.execute(
                    action_id=action_id,
                    input=input_data
                )
                
                # Wait for completion
                execution = await self.client.executions.wait_for_completion(
                    execution_id=result.id,
                    timeout=60
                )
                
                return execution
                
            except RateLimitError as e:
                logger.warning(f"Rate limit hit, waiting {e.retry_after} seconds")
                await asyncio.sleep(e.retry_after)
                attempt += 1
                continue
                
            except ValidationError as e:
                logger.error(f"Validation error: {e.errors}")
                # Don't retry validation errors
                raise
                
            except NotFoundError as e:
                logger.error(f"Action not found: {action_id}")
                raise
                
            except AuthenticationError as e:
                logger.error(f"Authentication failed: {e}")
                # Check if credentials need refresh
                raise
                
            except TolstoyError as e:
                logger.error(f"API error (attempt {attempt + 1}): {e}")
                if attempt == max_attempts - 1:
                    raise
                
                # Exponential backoff
                wait_time = (2 ** attempt) + (random.uniform(0, 1))
                await asyncio.sleep(wait_time)
                attempt += 1
                
        raise TolstoyError("Max retry attempts exceeded")

# Usage
async def main():
    service = TolstoyService("api-key", "org-id")
    
    try:
        result = await service.safe_execute_action(
            action_id="action_123",
            input_data={"user_id": "12345"}
        )
        print(f"Success: {result.output}")
    except Exception as e:
        logger.error(f"Failed to execute action: {e}")

asyncio.run(main())
```

## Advanced Workflow Patterns

### E-commerce Order Processing

```python
import asyncio
from typing import List, Dict, Any
from dataclasses import dataclass
from tolstoy import TolstoyClient

@dataclass
class OrderItem:
    product_id: str
    quantity: int
    price: float

@dataclass
class Order:
    order_id: str
    customer_email: str
    items: List[OrderItem]
    total_amount: float
    shipping_address: Dict[str, str]

class EcommerceOrderProcessor:
    def __init__(self, client: TolstoyClient):
        self.client = client
        
    async def process_order(self, order: Order) -> Dict[str, Any]:
        """Complete order processing workflow"""
        
        # Step 1: Validate inventory
        inventory_check = await self._check_inventory(order)
        if not inventory_check["available"]:
            raise ValueError(f"Insufficient inventory: {inventory_check['unavailable_items']}")
        
        # Step 2: Process payment
        payment_result = await self._process_payment(order)
        
        # Step 3: Update inventory (parallel)
        inventory_tasks = [
            self._update_inventory_item(item.product_id, -item.quantity)
            for item in order.items
        ]
        await asyncio.gather(*inventory_tasks)
        
        # Step 4: Send notifications (parallel)
        notification_tasks = [
            self._send_customer_confirmation(order, payment_result),
            self._notify_fulfillment_team(order),
            self._update_analytics(order)
        ]
        notifications = await asyncio.gather(*notification_tasks, return_exceptions=True)
        
        return {
            "order_id": order.order_id,
            "payment_id": payment_result["payment_intent_id"],
            "status": "processed",
            "notifications": [
                {"type": n.__class__.__name__ if isinstance(n, Exception) else "success", 
                 "result": str(n) if isinstance(n, Exception) else n}
                for n in notifications
            ]
        }
    
    async def _check_inventory(self, order: Order) -> Dict[str, Any]:
        """Check inventory for all items"""
        inventory_checks = []
        
        for item in order.items:
            result = await self.client.actions.execute(
                action_id="check_inventory",
                input={
                    "product_id": item.product_id,
                    "required_quantity": item.quantity
                }
            )
            inventory_checks.append({
                "product_id": item.product_id,
                "available": result.output.get("available", False),
                "current_stock": result.output.get("current_stock", 0)
            })
        
        unavailable_items = [
            check for check in inventory_checks 
            if not check["available"]
        ]
        
        return {
            "available": len(unavailable_items) == 0,
            "unavailable_items": unavailable_items,
            "checks": inventory_checks
        }
    
    async def _process_payment(self, order: Order) -> Dict[str, str]:
        """Process payment via Stripe"""
        payment_result = await self.client.actions.execute(
            action_id="process_stripe_payment",
            input={
                "amount": int(order.total_amount * 100),  # Convert to cents
                "currency": "usd",
                "customer_email": order.customer_email,
                "order_id": order.order_id,
                "description": f"Order {order.order_id}",
                "metadata": {
                    "order_id": order.order_id,
                    "items_count": len(order.items)
                }
            }
        )
        
        execution = await self.client.executions.wait_for_completion(
            payment_result.id, timeout=30
        )
        
        if execution.status != "success":
            raise RuntimeError(f"Payment failed: {execution.error}")
        
        return execution.output
    
    async def _update_inventory_item(self, product_id: str, quantity_delta: int):
        """Update inventory for a single item"""
        return await self.client.actions.execute(
            action_id="update_inventory",
            input={
                "product_id": product_id,
                "quantity_delta": quantity_delta,
                "reason": "order_fulfillment"
            }
        )
    
    async def _send_customer_confirmation(self, order: Order, payment_result: Dict):
        """Send order confirmation email to customer"""
        return await self.client.actions.execute(
            action_id="send_order_confirmation",
            input={
                "customer_email": order.customer_email,
                "order_id": order.order_id,
                "payment_id": payment_result["payment_intent_id"],
                "items": [
                    {
                        "product_id": item.product_id,
                        "quantity": item.quantity,
                        "price": item.price
                    }
                    for item in order.items
                ],
                "total_amount": order.total_amount,
                "shipping_address": order.shipping_address
            }
        )
    
    async def _notify_fulfillment_team(self, order: Order):
        """Notify fulfillment team via Slack"""
        return await self.client.actions.execute(
            action_id="send_slack_notification",
            input={
                "channel": "#fulfillment",
                "text": f"📦 New order to fulfill: {order.order_id}",
                "blocks": [
                    {
                        "type": "section",
                        "text": {
                            "type": "mrkdwn",
                            "text": f"*Order ID:* {order.order_id}\n*Customer:* {order.customer_email}\n*Items:* {len(order.items)}\n*Total:* ${order.total_amount:.2f}"
                        }
                    },
                    {
                        "type": "actions",
                        "elements": [
                            {
                                "type": "button",
                                "text": {"type": "plain_text", "text": "View Order"},
                                "url": f"https://admin.example.com/orders/{order.order_id}"
                            }
                        ]
                    }
                ]
            }
        )
    
    async def _update_analytics(self, order: Order):
        """Update analytics/tracking systems"""
        return await self.client.actions.execute(
            action_id="track_analytics_event",
            input={
                "event": "order_processed",
                "properties": {
                    "order_id": order.order_id,
                    "customer_email": order.customer_email,
                    "order_value": order.total_amount,
                    "items_count": len(order.items),
                    "timestamp": asyncio.get_event_loop().time()
                }
            }
        )

# Usage example
async def process_ecommerce_orders():
    client = TolstoyClient()
    processor = EcommerceOrderProcessor(client)
    
    sample_order = Order(
        order_id="ORD-12345",
        customer_email="customer@example.com",
        items=[
            OrderItem("PROD-1", 2, 29.99),
            OrderItem("PROD-2", 1, 49.99)
        ],
        total_amount=109.97,
        shipping_address={
            "street": "123 Main St",
            "city": "Anytown",
            "state": "CA",
            "zip": "90210"
        }
    )
    
    try:
        result = await processor.process_order(sample_order)
        print(f"✅ Order processed: {result}")
    except Exception as e:
        print(f"❌ Order processing failed: {e}")

asyncio.run(process_ecommerce_orders())
```

### Batch Processing with Concurrency Control

```python
import asyncio
from typing import List, Callable, Any, Dict
from dataclasses import dataclass
import time

@dataclass
class BatchResult:
    successful: List[Any]
    failed: List[Dict[str, Any]]
    total_time: float
    success_rate: float

class BatchProcessor:
    def __init__(self, client: TolstoyClient, max_concurrency: int = 10):
        self.client = client
        self.semaphore = asyncio.Semaphore(max_concurrency)
        
    async def process_batch(
        self,
        items: List[Any],
        processor: Callable,
        batch_size: int = 50,
        delay_between_batches: float = 1.0
    ) -> BatchResult:
        """Process items in batches with concurrency control"""
        
        start_time = time.time()
        all_successful = []
        all_failed = []
        
        # Split items into batches
        for i in range(0, len(items), batch_size):
            batch = items[i:i + batch_size]
            print(f"Processing batch {i//batch_size + 1}/{(len(items)-1)//batch_size + 1} ({len(batch)} items)")
            
            # Process batch with concurrency control
            batch_results = await asyncio.gather(
                *[self._process_single_item(item, processor) for item in batch],
                return_exceptions=True
            )
            
            # Categorize results
            for item, result in zip(batch, batch_results):
                if isinstance(result, Exception):
                    all_failed.append({
                        "item": item,
                        "error": str(result),
                        "error_type": type(result).__name__
                    })
                else:
                    all_successful.append(result)
            
            # Delay between batches to avoid overwhelming the API
            if i + batch_size < len(items):
                await asyncio.sleep(delay_between_batches)
        
        total_time = time.time() - start_time
        success_rate = len(all_successful) / len(items) * 100
        
        return BatchResult(
            successful=all_successful,
            failed=all_failed,
            total_time=total_time,
            success_rate=success_rate
        )
    
    async def _process_single_item(self, item: Any, processor: Callable) -> Any:
        """Process single item with semaphore control"""
        async with self.semaphore:
            return await processor(item)

# Example: Bulk user data processing
async def process_user_data(processor: BatchProcessor):
    """Process user data in batches"""
    
    # Sample user data
    users = [
        {"user_id": f"user_{i}", "email": f"user{i}@example.com"}
        for i in range(1, 101)
    ]
    
    async def sync_user_to_crm(user_data: Dict[str, str]) -> Dict[str, Any]:
        """Sync individual user to CRM"""
        result = await processor.client.actions.execute(
            action_id="sync_user_to_hubspot",
            input={
                "email": user_data["email"],
                "user_id": user_data["user_id"],
                "source": "batch_sync"
            }
        )
        
        # Wait for completion
        execution = await processor.client.executions.wait_for_completion(
            result.id, timeout=30
        )
        
        return {
            "user_id": user_data["user_id"],
            "email": user_data["email"],
            "crm_id": execution.output.get("contact_id"),
            "status": execution.status
        }
    
    # Process all users
    batch_result = await processor.process_batch(
        items=users,
        processor=sync_user_to_crm,
        batch_size=20,
        delay_between_batches=2.0
    )
    
    print(f"""
📊 Batch Processing Results:
✅ Successful: {len(batch_result.successful)}
❌ Failed: {len(batch_result.failed)}
🎯 Success Rate: {batch_result.success_rate:.1f}%
⏱️  Total Time: {batch_result.total_time:.2f}s
""")
    
    # Handle failures
    if batch_result.failed:
        print("\n❌ Failed Items:")
        for failure in batch_result.failed[:5]:  # Show first 5 failures
            print(f"  - {failure['item']['email']}: {failure['error']}")

# Usage
async def main():
    client = TolstoyClient()
    processor = BatchProcessor(client, max_concurrency=5)
    await process_user_data(processor)

asyncio.run(main())
```

### Real-time Data Synchronization

```python
import asyncio
import json
from datetime import datetime
from typing import Set, Dict, Any
from tolstoy import TolstoyClient

class DataSynchronizer:
    def __init__(self, client: TolstoyClient):
        self.client = client
        self.sync_queue = asyncio.Queue()
        self.processed_ids: Set[str] = set()
        self.is_running = False
        
    async def start_sync_worker(self):
        """Start the background sync worker"""
        self.is_running = True
        print("🔄 Starting data synchronization worker...")
        
        while self.is_running:
            try:
                # Get sync task from queue (wait up to 5 seconds)
                sync_task = await asyncio.wait_for(
                    self.sync_queue.get(), timeout=5.0
                )
                
                await self._process_sync_task(sync_task)
                self.sync_queue.task_done()
                
            except asyncio.TimeoutError:
                # No tasks in queue, continue loop
                continue
            except Exception as e:
                print(f"❌ Sync worker error: {e}")
                await asyncio.sleep(1)
        
        print("⏹️  Data synchronization worker stopped")
    
    async def queue_data_change(
        self, 
        entity_type: str, 
        entity_id: str, 
        change_type: str,
        data: Dict[str, Any]
    ):
        """Queue a data change for synchronization"""
        
        # Avoid duplicate processing
        task_id = f"{entity_type}:{entity_id}:{change_type}"
        if task_id in self.processed_ids:
            print(f"⏩ Skipping duplicate sync task: {task_id}")
            return
        
        sync_task = {
            "id": task_id,
            "entity_type": entity_type,
            "entity_id": entity_id,
            "change_type": change_type,
            "data": data,
            "timestamp": datetime.now().isoformat(),
            "retry_count": 0
        }
        
        await self.sync_queue.put(sync_task)
        print(f"📝 Queued sync task: {task_id}")
    
    async def _process_sync_task(self, sync_task: Dict[str, Any]):
        """Process a single synchronization task"""
        task_id = sync_task["id"]
        
        try:
            print(f"🔄 Processing sync task: {task_id}")
            
            # Route to appropriate sync handler
            if sync_task["entity_type"] == "user":
                await self._sync_user_change(sync_task)
            elif sync_task["entity_type"] == "order":
                await self._sync_order_change(sync_task)
            elif sync_task["entity_type"] == "product":
                await self._sync_product_change(sync_task)
            else:
                print(f"❓ Unknown entity type: {sync_task['entity_type']}")
                return
            
            # Mark as processed
            self.processed_ids.add(task_id)
            print(f"✅ Completed sync task: {task_id}")
            
        except Exception as e:
            print(f"❌ Failed sync task: {task_id} - {e}")
            
            # Retry logic
            sync_task["retry_count"] += 1
            if sync_task["retry_count"] < 3:
                # Re-queue with exponential backoff
                delay = 2 ** sync_task["retry_count"]
                await asyncio.sleep(delay)
                await self.sync_queue.put(sync_task)
                print(f"🔄 Retrying sync task: {task_id} (attempt {sync_task['retry_count'] + 1})")
            else:
                print(f"💀 Max retries exceeded for: {task_id}")
    
    async def _sync_user_change(self, sync_task: Dict[str, Any]):
        """Sync user changes to external systems"""
        data = sync_task["data"]
        change_type = sync_task["change_type"]
        
        # Prepare sync data based on change type
        if change_type == "created":
            # Sync to CRM (HubSpot)
            await self.client.flows.execute(
                flow_id="sync_new_user_to_hubspot",
                input={
                    "user_id": sync_task["entity_id"],
                    "email": data["email"],
                    "name": data.get("name", ""),
                    "signup_source": data.get("source", "unknown"),
                    "created_at": sync_task["timestamp"]
                }
            )
            
            # Sync to email marketing (SendGrid)
            await self.client.flows.execute(
                flow_id="add_user_to_email_list",
                input={
                    "email": data["email"],
                    "list_id": "new_users",
                    "custom_fields": {
                        "signup_date": sync_task["timestamp"],
                        "user_id": sync_task["entity_id"]
                    }
                }
            )
            
        elif change_type == "updated":
            # Sync profile updates
            await self.client.flows.execute(
                flow_id="update_user_profile",
                input={
                    "user_id": sync_task["entity_id"],
                    "updated_fields": data,
                    "updated_at": sync_task["timestamp"]
                }
            )
        
        elif change_type == "deleted":
            # Handle user deletion
            await self.client.flows.execute(
                flow_id="handle_user_deletion",
                input={
                    "user_id": sync_task["entity_id"],
                    "deleted_at": sync_task["timestamp"],
                    "cleanup_external_data": True
                }
            )
    
    async def _sync_order_change(self, sync_task: Dict[str, Any]):
        """Sync order changes to external systems"""
        # Similar pattern for orders
        pass
    
    async def _sync_product_change(self, sync_task: Dict[str, Any]):
        """Sync product changes to external systems"""
        # Similar pattern for products  
        pass
    
    async def stop_sync_worker(self):
        """Stop the sync worker gracefully"""
        self.is_running = False
        
        # Wait for remaining tasks to complete
        await self.sync_queue.join()

# Usage example with webhook integration
from fastapi import FastAPI, BackgroundTasks

app = FastAPI()
client = TolstoyClient()
syncer = DataSynchronizer(client)

@app.on_event("startup")
async def startup_event():
    # Start sync worker in background
    asyncio.create_task(syncer.start_sync_worker())

@app.on_event("shutdown") 
async def shutdown_event():
    await syncer.stop_sync_worker()

@app.post("/webhooks/user-created")
async def handle_user_created(user_data: dict, background_tasks: BackgroundTasks):
    """Handle user creation webhook"""
    
    # Queue sync task
    await syncer.queue_data_change(
        entity_type="user",
        entity_id=user_data["id"],
        change_type="created",
        data=user_data
    )
    
    return {"status": "queued"}

@app.post("/webhooks/user-updated")
async def handle_user_updated(update_data: dict):
    """Handle user update webhook"""
    
    await syncer.queue_data_change(
        entity_type="user", 
        entity_id=update_data["user_id"],
        change_type="updated",
        data=update_data["changes"]
    )
    
    return {"status": "queued"}

# Run with: uvicorn sync_server:app --reload
```

### Monitoring and Observability

```python
import asyncio
import time
import logging
from typing import Dict, List, Optional
from dataclasses import dataclass, field
from collections import defaultdict
from tolstoy import TolstoyClient

@dataclass
class ExecutionMetrics:
    execution_id: str
    flow_id: str
    status: str
    start_time: float
    end_time: Optional[float] = None
    duration: Optional[float] = None
    error: Optional[str] = None
    retry_count: int = 0

@dataclass 
class FlowMetrics:
    flow_id: str
    executions: List[ExecutionMetrics] = field(default_factory=list)
    total_executions: int = 0
    successful_executions: int = 0
    failed_executions: int = 0
    average_duration: float = 0.0
    error_rate: float = 0.0

class WorkflowMonitor:
    def __init__(self, client: TolstoyClient):
        self.client = client
        self.metrics: Dict[str, FlowMetrics] = defaultdict(lambda: FlowMetrics(flow_id=""))
        self.active_executions: Dict[str, ExecutionMetrics] = {}
        self.logger = logging.getLogger(__name__)
        
    async def start_monitoring(self, poll_interval: int = 30):
        """Start monitoring workflow executions"""
        self.logger.info("🔍 Starting workflow monitoring...")
        
        while True:
            try:
                await self._poll_executions()
                await self._update_metrics()
                await self._check_alerts()
                await asyncio.sleep(poll_interval)
                
            except Exception as e:
                self.logger.error(f"Monitoring error: {e}")
                await asyncio.sleep(poll_interval)
    
    async def _poll_executions(self):
        """Poll for active and recent executions"""
        # Get recent executions
        executions = await self.client.execution_logs.list(
            limit=100,
            status=["running", "completed", "failed"],
            since=time.time() - 300  # Last 5 minutes
        )
        
        for exec_data in executions.data:
            execution_id = exec_data["id"]
            
            if execution_id not in self.active_executions:
                # New execution
                metrics = ExecutionMetrics(
                    execution_id=execution_id,
                    flow_id=exec_data["flow_id"],
                    status=exec_data["status"],
                    start_time=exec_data["started_at"]
                )
                self.active_executions[execution_id] = metrics
                
            else:
                # Update existing execution
                metrics = self.active_executions[execution_id]
                metrics.status = exec_data["status"]
                
                if exec_data["status"] in ["completed", "failed"]:
                    metrics.end_time = exec_data.get("completed_at", time.time())
                    metrics.duration = metrics.end_time - metrics.start_time
                    
                    if exec_data["status"] == "failed":
                        metrics.error = exec_data.get("error", {}).get("message", "Unknown error")
    
    async def _update_metrics(self):
        """Update aggregated metrics for each flow"""
        for execution in self.active_executions.values():
            if execution.status not in ["completed", "failed"]:
                continue
                
            flow_metrics = self.metrics[execution.flow_id]
            flow_metrics.flow_id = execution.flow_id
            
            # Add to executions list
            if execution not in flow_metrics.executions:
                flow_metrics.executions.append(execution)
                flow_metrics.total_executions += 1
                
                if execution.status == "completed":
                    flow_metrics.successful_executions += 1
                elif execution.status == "failed":
                    flow_metrics.failed_executions += 1
                
                # Calculate averages
                completed_executions = [e for e in flow_metrics.executions if e.duration]
                if completed_executions:
                    flow_metrics.average_duration = sum(e.duration for e in completed_executions) / len(completed_executions)
                
                # Calculate error rate
                if flow_metrics.total_executions > 0:
                    flow_metrics.error_rate = (flow_metrics.failed_executions / flow_metrics.total_executions) * 100
    
    async def _check_alerts(self):
        """Check for alert conditions and send notifications"""
        for flow_id, metrics in self.metrics.items():
            if metrics.total_executions < 5:  # Need minimum data
                continue
            
            # High error rate alert
            if metrics.error_rate > 20:  # > 20% error rate
                await self._send_alert(
                    severity="high",
                    message=f"High error rate for flow {flow_id}: {metrics.error_rate:.1f}%",
                    flow_id=flow_id,
                    metric_type="error_rate",
                    value=metrics.error_rate
                )
            
            # Slow execution alert
            if metrics.average_duration > 300:  # > 5 minutes
                await self._send_alert(
                    severity="medium",
                    message=f"Slow execution for flow {flow_id}: {metrics.average_duration:.1f}s average",
                    flow_id=flow_id,
                    metric_type="duration",
                    value=metrics.average_duration
                )
    
    async def _send_alert(self, severity: str, message: str, flow_id: str, metric_type: str, value: float):
        """Send alert notification"""
        try:
            await self.client.actions.execute(
                action_id="send_monitoring_alert",
                input={
                    "severity": severity,
                    "message": message,
                    "flow_id": flow_id,
                    "metric_type": metric_type,
                    "metric_value": value,
                    "timestamp": time.time(),
                    "dashboard_url": f"https://app.tolstoy.com/flows/{flow_id}/metrics"
                }
            )
            
            self.logger.warning(f"🚨 Alert sent: {message}")
            
        except Exception as e:
            self.logger.error(f"Failed to send alert: {e}")
    
    def get_flow_summary(self, flow_id: str) -> Optional[Dict]:
        """Get metrics summary for a specific flow"""
        if flow_id not in self.metrics:
            return None
        
        metrics = self.metrics[flow_id]
        recent_executions = [e for e in metrics.executions if time.time() - e.start_time < 3600]  # Last hour
        
        return {
            "flow_id": flow_id,
            "total_executions": metrics.total_executions,
            "successful_executions": metrics.successful_executions,
            "failed_executions": metrics.failed_executions,
            "error_rate": metrics.error_rate,
            "average_duration": metrics.average_duration,
            "executions_last_hour": len(recent_executions),
            "recent_errors": [
                {
                    "execution_id": e.execution_id,
                    "error": e.error,
                    "timestamp": e.start_time
                }
                for e in recent_executions 
                if e.status == "failed"
            ][-5:]  # Last 5 errors
        }
    
    def get_system_health(self) -> Dict:
        """Get overall system health metrics"""
        total_executions = sum(m.total_executions for m in self.metrics.values())
        total_failed = sum(m.failed_executions for m in self.metrics.values())
        overall_error_rate = (total_failed / total_executions * 100) if total_executions > 0 else 0
        
        active_flows = len([m for m in self.metrics.values() if m.total_executions > 0])
        
        # Recent activity (last hour)
        one_hour_ago = time.time() - 3600
        recent_executions = []
        for metrics in self.metrics.values():
            recent_executions.extend([e for e in metrics.executions if e.start_time > one_hour_ago])
        
        return {
            "active_flows": active_flows,
            "total_executions": total_executions,
            "overall_error_rate": overall_error_rate,
            "executions_last_hour": len(recent_executions),
            "currently_running": len(self.active_executions),
            "flows_with_errors": len([m for m in self.metrics.values() if m.error_rate > 0]),
            "average_execution_time": sum(m.average_duration for m in self.metrics.values()) / max(len(self.metrics), 1)
        }

# Usage
async def monitoring_example():
    client = TolstoyClient()
    monitor = WorkflowMonitor(client)
    
    # Start monitoring in background
    monitor_task = asyncio.create_task(monitor.start_monitoring(poll_interval=30))
    
    # Simulate checking metrics every 5 minutes
    while True:
        await asyncio.sleep(300)  # 5 minutes
        
        # Get system health
        health = monitor.get_system_health()
        print(f"""
📊 System Health Report:
  Active Flows: {health['active_flows']}
  Executions (last hour): {health['executions_last_hour']}
  Overall Error Rate: {health['overall_error_rate']:.1f}%
  Currently Running: {health['currently_running']}
""")

# Run monitoring
# asyncio.run(monitoring_example())
```

This comprehensive set of Python SDK examples demonstrates advanced patterns for production use of the Tolstoy platform. Each example includes proper error handling, logging, monitoring, and follows Python best practices.