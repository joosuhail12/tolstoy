---
title: "List Execution Logs"
openapi: "GET /execution-logs"
---

# List Execution Logs

Retrieve detailed logs for flow and action executions with comprehensive filtering, search, and pagination capabilities. This endpoint provides complete execution history and debugging information.

<RequestExample>

```bash cURL
curl -X GET "https://tolstoy.getpullse.com/execution-logs?limit=50&level=error&flow_id=flow_abc123" \
  -H "x-org-id: org_1234567890abcdef" \
  -H "x-user-id: user_abcdef1234567890" \
  -H "Content-Type: application/json"
```

```typescript TypeScript
interface ListExecutionLogsParams {
  limit?: number;
  cursor?: string;
  flow_id?: string;
  execution_id?: string;
  level?: 'debug' | 'info' | 'warn' | 'error';
  start_time?: string;
  end_time?: string;
  search?: string;
}

const params: ListExecutionLogsParams = {
  limit: 50,
  level: 'error',
  flow_id: 'flow_abc123def456',
  start_time: '2024-01-15T00:00:00Z',
  end_time: '2024-01-15T23:59:59Z'
};

const searchParams = new URLSearchParams();
Object.entries(params).forEach(([key, value]) => {
  if (value !== undefined) {
    searchParams.set(key, value.toString());
  }
});

const response = await fetch(`https://tolstoy.getpullse.com/execution-logs?${searchParams}`, {
  method: 'GET',
  headers: {
    'x-org-id': 'org_1234567890abcdef',
    'x-user-id': 'user_abcdef1234567890',
    'Content-Type': 'application/json'
  }
});

const logs = await response.json();
if (logs.success) {
  console.log(`Found ${logs.data.length} log entries`);
  logs.data.forEach(log => {
    console.log(`[${log.level}] ${log.timestamp}: ${log.message}`);
  });
}
```

```python Python
import requests
from datetime import datetime, timedelta
from typing import Optional, List, Dict, Any

class ExecutionLogsClient:
    def __init__(self, org_id: str, user_id: str):
        self.org_id = org_id
        self.user_id = user_id
        self.headers = {
            'x-org-id': org_id,
            'x-user-id': user_id,
            'Content-Type': 'application/json'
        }
    
    def list_execution_logs(
        self,
        flow_id: Optional[str] = None,
        execution_id: Optional[str] = None,
        level: Optional[str] = None,
        start_time: Optional[str] = None,
        end_time: Optional[str] = None,
        search: Optional[str] = None,
        limit: int = 50
    ) -> List[Dict[str, Any]]:
        params = {
            'limit': limit
        }
        
        # Add optional parameters
        if flow_id:
            params['flow_id'] = flow_id
        if execution_id:
            params['execution_id'] = execution_id
        if level:
            params['level'] = level
        if start_time:
            params['start_time'] = start_time
        if end_time:
            params['end_time'] = end_time
        if search:
            params['search'] = search
        
        response = requests.get(
            'https://tolstoy.getpullse.com/execution-logs',
            headers=self.headers,
            params=params
        )
        response.raise_for_status()
        
        result = response.json()
        if result['success']:
            return result['data']
        else:
            raise Exception(f"Failed to fetch logs: {result['error']['message']}")
    
    def get_error_logs(self, hours_ago: int = 24) -> List[Dict[str, Any]]:
        """Get error logs from the last N hours"""
        end_time = datetime.utcnow()
        start_time = end_time - timedelta(hours=hours_ago)
        
        return self.list_execution_logs(
            level='error',
            start_time=start_time.isoformat() + 'Z',
            end_time=end_time.isoformat() + 'Z',
            limit=100
        )
    
    def get_flow_logs(self, flow_id: str) -> List[Dict[str, Any]]:
        """Get all logs for a specific flow"""
        return self.list_execution_logs(flow_id=flow_id, limit=100)

# Usage
client = ExecutionLogsClient('org_1234567890abcdef', 'user_abcdef1234567890')

# Get recent error logs
error_logs = client.get_error_logs(hours_ago=6)
print(f"Found {len(error_logs)} error logs in the last 6 hours")

# Get logs for a specific flow
flow_logs = client.get_flow_logs('flow_abc123def456')
for log in flow_logs[:5]:  # Show first 5 entries
    print(f"[{log['level'].upper()}] {log['message']}")
```

```javascript Node.js
const axios = require('axios');

class ExecutionLogsAnalyzer {
  constructor(orgId, userId) {
    this.orgId = orgId;
    this.userId = userId;
    this.headers = {
      'x-org-id': orgId,
      'x-user-id': userId,
      'Content-Type': 'application/json'
    };
  }

  async getExecutionLogs(options = {}) {
    const params = new URLSearchParams({
      limit: 50,
      ...options
    });

    try {
      const response = await axios.get(
        `https://tolstoy.getpullse.com/execution-logs?${params}`,
        { headers: this.headers }
      );

      if (!response.data.success) {
        throw new Error(response.data.error.message);
      }

      return response.data.data;
    } catch (error) {
      console.error('Failed to fetch execution logs:', error.message);
      throw error;
    }
  }

  async analyzeErrorTrends(hoursBack = 24) {
    const endTime = new Date();
    const startTime = new Date(endTime.getTime() - (hoursBack * 60 * 60 * 1000));

    const errorLogs = await this.getExecutionLogs({
      level: 'error',
      start_time: startTime.toISOString(),
      end_time: endTime.toISOString(),
      limit: 200
    });

    // Group errors by type
    const errorGroups = {};
    errorLogs.forEach(log => {
      const errorType = log.metadata?.error_code || 'UNKNOWN_ERROR';
      if (!errorGroups[errorType]) {
        errorGroups[errorType] = [];
      }
      errorGroups[errorType].push(log);
    });

    // Analyze trends
    const analysis = Object.entries(errorGroups).map(([errorType, logs]) => ({
      error_type: errorType,
      count: logs.length,
      first_occurrence: logs[0]?.timestamp,
      last_occurrence: logs[logs.length - 1]?.timestamp,
      affected_flows: [...new Set(logs.map(log => log.flow_id).filter(Boolean))]
    }));

    return analysis.sort((a, b) => b.count - a.count);
  }

  async getFlowExecutionTrace(executionId) {
    const logs = await this.getExecutionLogs({
      execution_id: executionId,
      limit: 500
    });

    // Sort chronologically
    const sortedLogs = logs.sort((a, b) => 
      new Date(a.timestamp).getTime() - new Date(b.timestamp).getTime()
    );

    return {
      execution_id: executionId,
      total_logs: sortedLogs.length,
      duration_ms: sortedLogs.length > 1 ? 
        new Date(sortedLogs[sortedLogs.length - 1].timestamp).getTime() - 
        new Date(sortedLogs[0].timestamp).getTime() : 0,
      logs: sortedLogs
    };
  }
}

// Usage example
const analyzer = new ExecutionLogsAnalyzer('org_1234567890abcdef', 'user_abcdef1234567890');

// Analyze error trends
analyzer.analyzeErrorTrends(6)
  .then(trends => {
    console.log('Error Trends (Last 6 hours):');
    trends.forEach(trend => {
      console.log(`${trend.error_type}: ${trend.count} occurrences`);
      console.log(`  Affected flows: ${trend.affected_flows.length}`);
    });
  });

// Get full execution trace
analyzer.getFlowExecutionTrace('exec_abc123def456')
  .then(trace => {
    console.log(`\nExecution Trace: ${trace.execution_id}`);
    console.log(`Total logs: ${trace.total_logs}`);
    console.log(`Duration: ${trace.duration_ms}ms`);
    
    trace.logs.slice(0, 10).forEach(log => {
      console.log(`[${log.level}] ${log.message}`);
    });
  });
```

</RequestExample>

## Query Parameters

<ParamField query="limit" type="integer" default="50">
Number of log entries to return per page. Maximum value is 500.
</ParamField>

<ParamField query="cursor" type="string">
Pagination cursor for retrieving the next page of results.
</ParamField>

<ParamField query="flow_id" type="string">
Filter logs by specific flow ID (e.g., `flow_abc123def456`).
</ParamField>

<ParamField query="execution_id" type="string">
Filter logs by specific execution ID (e.g., `exec_abc123def456`).
</ParamField>

<ParamField query="action_id" type="string">
Filter logs by specific action type (e.g., `email-send`, `http-request`).
</ParamField>

<ParamField query="level" type="string">
Filter by log level. Available values:
- `debug` - Detailed debugging information
- `info` - General information messages
- `warn` - Warning messages for potential issues
- `error` - Error messages for failed operations
</ParamField>

<ParamField query="start_time" type="string">
Filter logs from this timestamp onwards (ISO 8601 format, e.g., `2024-01-15T00:00:00Z`).
</ParamField>

<ParamField query="end_time" type="string">
Filter logs up to this timestamp (ISO 8601 format, e.g., `2024-01-15T23:59:59Z`).
</ParamField>

<ParamField query="search" type="string">
Search logs by message content. Supports full-text search and is case-insensitive.
</ParamField>

<ParamField query="step_key" type="string">
Filter logs by specific step key within flows (e.g., `send_email`, `process_payment`).
</ParamField>

<ParamField query="user_id" type="string">
Filter logs by user who triggered the execution.
</ParamField>

<ParamField query="sort_order" type="string" default="desc">
Sort direction for logs by timestamp: `asc` (oldest first) or `desc` (newest first).
</ParamField>

## Response

<ResponseExample>

```json Success Response
{
  "success": true,
  "data": [
    {
      "id": "log_abc123def456789",
      "execution_id": "exec_flow123abc456",
      "flow_id": "flow_customer_onboarding",
      "step_key": "send_welcome_email",
      "action_id": "email-send",
      "level": "info",
      "message": "Email sent successfully to customer@example.com",
      "timestamp": "2024-01-15T14:30:15.123Z",
      "duration_ms": 1250,
      "metadata": {
        "email_provider": "sendgrid",
        "message_id": "msg_xyz789",
        "recipient": "customer@example.com",
        "template_used": "welcome-template"
      },
      "context": {
        "organization_id": "org_1234567890abcdef",
        "user_id": "user_abcdef1234567890",
        "execution_mode": "sync",
        "flow_version": 2
      }
    },
    {
      "id": "log_def456ghi789012", 
      "execution_id": "exec_flow123abc456",
      "flow_id": "flow_customer_onboarding",
      "step_key": "create_account",
      "action_id": "http-request",
      "level": "error",
      "message": "HTTP request failed: Connection timeout to api.crm.com",
      "timestamp": "2024-01-15T14:30:20.456Z",
      "duration_ms": 30000,
      "metadata": {
        "error_code": "CONNECTION_TIMEOUT",
        "url": "https://api.crm.com/accounts",
        "method": "POST",
        "status_code": null,
        "retry_attempt": 2,
        "max_retries": 3
      },
      "context": {
        "organization_id": "org_1234567890abcdef",
        "user_id": "user_abcdef1234567890",
        "execution_mode": "sync",
        "flow_version": 2
      },
      "stack_trace": [
        "at HttpRequest.execute (/app/actions/http-request.js:45:12)",
        "at FlowExecutor.executeStep (/app/flow-executor.js:128:23)",
        "at FlowExecutor.run (/app/flow-executor.js:89:15)"
      ]
    },
    {
      "id": "log_ghi789012345678",
      "execution_id": null,
      "flow_id": null,
      "step_key": null,
      "action_id": "slack-message",
      "level": "warn",
      "message": "Slack rate limit approaching: 45/50 requests in current minute",
      "timestamp": "2024-01-15T14:29:45.789Z",
      "duration_ms": null,
      "metadata": {
        "rate_limit_info": {
          "current": 45,
          "limit": 50,
          "window_reset": "2024-01-15T14:30:00Z"
        },
        "workspace": "company-workspace"
      },
      "context": {
        "organization_id": "org_1234567890abcdef",
        "user_id": null,
        "execution_mode": null,
        "source": "rate_limiter"
      }
    },
    {
      "id": "log_jkl012345678901",
      "execution_id": "exec_standalone_email",
      "flow_id": null,
      "step_key": null,
      "action_id": "email-send",
      "level": "debug",
      "message": "Email template rendered with variables",
      "timestamp": "2024-01-15T14:28:30.234Z",
      "duration_ms": 45,
      "metadata": {
        "template_id": "customer-welcome",
        "variables_count": 4,
        "variables": {
          "customer_name": "John Smith",
          "plan_type": "Premium",
          "trial_days": 14,
          "support_email": "support@company.com"
        },
        "rendered_subject": "Welcome to Premium Plan, John Smith!",
        "content_length": 2847
      },
      "context": {
        "organization_id": "org_1234567890abcdef", 
        "user_id": "user_abcdef1234567890",
        "execution_mode": "sync",
        "source": "standalone_action"
      }
    }
  ],
  "pagination": {
    "limit": 50,
    "cursor": "eyJ0aW1lc3RhbXAiOiIyMDI0LTAxLTE1VDE0OjI4OjMwLjIzNFoiLCJpZCI6ImxvZ19qa2wwMTIzNDU2Nzg5MDEifQ",
    "has_more": true,
    "total_count": 1247
  },
  "metadata": {
    "request_id": "req_logs123def456",
    "timestamp": "2024-01-15T14:35:00Z",
    "execution_time_ms": 89,
    "filters_applied": {
      "level": null,
      "flow_id": null,
      "start_time": null,
      "end_time": null
    }
  }
}
```

```json Filtered Error Logs
{
  "success": true,
  "data": [
    {
      "id": "log_error123abc456",
      "execution_id": "exec_failed_payment",
      "flow_id": "flow_payment_process",
      "step_key": "charge_customer",
      "action_id": "http-request",
      "level": "error",
      "message": "Payment processing failed: Insufficient funds",
      "timestamp": "2024-01-15T14:25:00.123Z",
      "duration_ms": 2340,
      "metadata": {
        "error_code": "INSUFFICIENT_FUNDS",
        "payment_method": "card_ending_1234",
        "amount": 29.99,
        "currency": "USD",
        "transaction_id": "txn_abc123",
        "provider_response": {
          "code": "card_declined",
          "decline_code": "insufficient_funds",
          "message": "Your card has insufficient funds."
        }
      },
      "context": {
        "organization_id": "org_1234567890abcdef",
        "user_id": "user_customer123",
        "execution_mode": "async",
        "customer_id": "cust_john_smith_456"
      }
    },
    {
      "id": "log_error456def789",
      "execution_id": "exec_webhook_failed",
      "flow_id": "flow_order_fulfillment", 
      "step_key": "notify_warehouse",
      "action_id": "http-request",
      "level": "error",
      "message": "Webhook delivery failed after 3 retry attempts",
      "timestamp": "2024-01-15T14:20:30.456Z",
      "duration_ms": 90000,
      "metadata": {
        "error_code": "WEBHOOK_DELIVERY_FAILED",
        "webhook_url": "https://warehouse.company.com/webhook",
        "final_status_code": 503,
        "retry_attempts": 3,
        "retry_schedule": ["1s", "4s", "16s"],
        "last_error": "Service temporarily unavailable"
      },
      "context": {
        "organization_id": "org_1234567890abcdef",
        "user_id": "user_order_system",
        "execution_mode": "async",
        "order_id": "ord_789012"
      },
      "stack_trace": [
        "at WebhookDelivery.sendWithRetry (/app/webhook-delivery.js:78:15)",
        "at HttpRequest.execute (/app/actions/http-request.js:67:21)", 
        "at FlowExecutor.executeStep (/app/flow-executor.js:145:18)"
      ]
    }
  ],
  "pagination": {
    "limit": 50,
    "cursor": null,
    "has_more": false,
    "total_count": 2
  },
  "metadata": {
    "request_id": "req_error_logs_789",
    "timestamp": "2024-01-15T14:35:00Z",
    "execution_time_ms": 45,
    "filters_applied": {
      "level": "error",
      "start_time": "2024-01-15T00:00:00Z",
      "end_time": "2024-01-15T23:59:59Z"
    }
  }
}
```

```json Empty Results
{
  "success": true,
  "data": [],
  "pagination": {
    "limit": 50,
    "cursor": null,
    "has_more": false,
    "total_count": 0
  },
  "metadata": {
    "request_id": "req_no_logs_123",
    "timestamp": "2024-01-15T14:35:00Z",
    "execution_time_ms": 12,
    "filters_applied": {
      "flow_id": "flow_nonexistent",
      "level": null
    }
  }
}
```

</ResponseExample>

## Response Fields

### Log Entry Object

<ResponseField name="id" type="string">
Unique log entry identifier
</ResponseField>

<ResponseField name="execution_id" type="string | null">
ID of the execution that generated this log (null for system logs)
</ResponseField>

<ResponseField name="flow_id" type="string | null">
ID of the flow that generated this log (null for standalone actions or system logs)
</ResponseField>

<ResponseField name="step_key" type="string | null">
Key of the specific step that generated this log (null for flow-level or system logs)
</ResponseField>

<ResponseField name="action_id" type="string | null">
ID of the action that generated this log (null for flow-level or system logs)
</ResponseField>

<ResponseField name="level" type="string">
Log level: `debug`, `info`, `warn`, or `error`
</ResponseField>

<ResponseField name="message" type="string">
Human-readable log message describing what happened
</ResponseField>

<ResponseField name="timestamp" type="string">
ISO 8601 timestamp when the log entry was created
</ResponseField>

<ResponseField name="duration_ms" type="integer | null">
Time taken for the operation in milliseconds (null if not applicable)
</ResponseField>

<ResponseField name="metadata" type="object">
Additional structured data related to the log entry
  <Expandable title="Common Metadata Fields">
    <ResponseField name="error_code" type="string">Error code for error logs</ResponseField>
    <ResponseField name="*" type="any">Action-specific or context-specific metadata</ResponseField>
  </Expandable>
</ResponseField>

<ResponseField name="context" type="object">
Execution context information
  <Expandable title="Context Fields">
    <ResponseField name="organization_id" type="string">Organization that owns this execution</ResponseField>
    <ResponseField name="user_id" type="string | null">User who triggered the execution</ResponseField>
    <ResponseField name="execution_mode" type="string | null">Execution mode: `sync` or `async`</ResponseField>
    <ResponseField name="flow_version" type="integer">Flow version (if applicable)</ResponseField>
    <ResponseField name="source" type="string">Source of the log entry</ResponseField>
  </Expandable>
</ResponseField>

<ResponseField name="stack_trace" type="string[]">
Stack trace for error logs (when available)
</ResponseField>

## Status Codes

<ResponseField name="200" type="Success">
Logs retrieved successfully
</ResponseField>

<ResponseField name="400" type="Bad Request">
Invalid query parameters or date range
</ResponseField>

<ResponseField name="401" type="Unauthorized">
Missing or invalid authentication
</ResponseField>

<ResponseField name="429" type="Too Many Requests">
Rate limit exceeded
</ResponseField>

<ResponseField name="500" type="Internal Server Error">
Server error occurred
</ResponseField>

## Examples

### Error Monitoring

<CodeGroup>

```typescript Real-time Error Monitoring
async function monitorErrors() {
  const lastHour = new Date(Date.now() - 60 * 60 * 1000).toISOString();
  
  const errorLogs = await fetch('/execution-logs?' + new URLSearchParams({
    level: 'error',
    start_time: lastHour,
    limit: '100'
  }), {
    headers: {
      'x-org-id': 'org_1234567890abcdef',
      'x-user-id': 'user_abcdef1234567890'
    }
  }).then(r => r.json());
  
  if (errorLogs.success) {
    const errorsByFlow = {};
    errorLogs.data.forEach(log => {
      const flowId = log.flow_id || 'standalone';
      if (!errorsByFlow[flowId]) {
        errorsByFlow[flowId] = [];
      }
      errorsByFlow[flowId].push(log);
    });
    
    Object.entries(errorsByFlow).forEach(([flowId, errors]) => {
      console.log(`${flowId}: ${errors.length} errors`);
      errors.slice(0, 3).forEach(error => {
        console.log(`  - ${error.message}`);
      });
    });
  }
}
```

```python Log Analysis
import requests
from collections import defaultdict, Counter
from datetime import datetime, timedelta

class LogAnalyzer:
    def __init__(self, org_id: str, user_id: str):
        self.headers = {
            'x-org-id': org_id,
            'x-user-id': user_id,
            'Content-Type': 'application/json'
        }
    
    def analyze_error_patterns(self, hours_back: int = 24):
        """Analyze error patterns over time"""
        start_time = datetime.utcnow() - timedelta(hours=hours_back)
        
        response = requests.get(
            'https://tolstoy.getpullse.com/execution-logs',
            headers=self.headers,
            params={
                'level': 'error',
                'start_time': start_time.isoformat() + 'Z',
                'limit': 500
            }
        )
        
        logs = response.json()['data']
        
        # Group by error code
        error_codes = Counter(log['metadata'].get('error_code', 'UNKNOWN') for log in logs)
        
        # Group by flow
        flow_errors = defaultdict(int)
        for log in logs:
            if log['flow_id']:
                flow_errors[log['flow_id']] += 1
        
        # Group by hour
        hourly_errors = defaultdict(int)
        for log in logs:
            hour = datetime.fromisoformat(log['timestamp'].replace('Z', '+00:00')).hour
            hourly_errors[hour] += 1
        
        return {
            'total_errors': len(logs),
            'error_codes': dict(error_codes.most_common(10)),
            'flows_with_errors': dict(sorted(flow_errors.items(), key=lambda x: x[1], reverse=True)),
            'errors_by_hour': dict(hourly_errors)
        }

# Usage
analyzer = LogAnalyzer('org_1234567890abcdef', 'user_abcdef1234567890')
analysis = analyzer.analyze_error_patterns(hours_back=6)

print(f"Total errors in last 6 hours: {analysis['total_errors']}")
print("\nTop error codes:")
for code, count in list(analysis['error_codes'].items())[:5]:
    print(f"  {code}: {count}")

print("\nFlows with most errors:")
for flow, count in list(analysis['flows_with_errors'].items())[:5]:
    print(f"  {flow}: {count}")
```

</CodeGroup>

### Performance Analysis

<CodeGroup>

```javascript Performance Tracking
async function analyzePerformance(flowId) {
  const logs = await fetch('/execution-logs?' + new URLSearchParams({
    flow_id: flowId,
    level: 'info',
    limit: '200'
  }), {
    headers: {
      'x-org-id': 'org_1234567890abcdef',
      'x-user-id': 'user_abcdef1234567890'
    }
  }).then(r => r.json());
  
  if (!logs.success) return null;
  
  // Analyze step performance
  const stepPerformance = {};
  logs.data.forEach(log => {
    if (log.step_key && log.duration_ms) {
      if (!stepPerformance[log.step_key]) {
        stepPerformance[log.step_key] = [];
      }
      stepPerformance[log.step_key].push(log.duration_ms);
    }
  });
  
  // Calculate statistics
  const stats = {};
  Object.entries(stepPerformance).forEach(([step, durations]) => {
    durations.sort((a, b) => a - b);
    const avg = durations.reduce((a, b) => a + b, 0) / durations.length;
    const median = durations[Math.floor(durations.length / 2)];
    const p95 = durations[Math.floor(durations.length * 0.95)];
    
    stats[step] = {
      count: durations.length,
      avg: Math.round(avg),
      median,
      p95,
      min: durations[0],
      max: durations[durations.length - 1]
    };
  });
  
  return stats;
}

// Usage
analyzePerformance('flow_customer_onboarding')
  .then(stats => {
    console.log('Step Performance Analysis:');
    Object.entries(stats).forEach(([step, metrics]) => {
      console.log(`\n${step}:`);
      console.log(`  Executions: ${metrics.count}`);
      console.log(`  Avg: ${metrics.avg}ms`);
      console.log(`  P95: ${metrics.p95}ms`);
    });
  });
```

</CodeGroup>

## Best Practices

### Efficient Querying

<Accordion title="Time Range Optimization">
- Always specify time ranges for better query performance
- Use reasonable time windows (avoid querying months of data at once)
- Consider pagination for large result sets
- Use specific filters (flow_id, execution_id) when possible
</Accordion>

<Accordion title="Log Level Usage">
- Use `error` level for alerting and monitoring
- Use `warn` level for performance and capacity monitoring  
- Use `info` level for operational visibility
- Use `debug` level only for troubleshooting specific issues
</Accordion>

### Monitoring and Alerting

<Accordion title="Error Detection">
- Monitor error logs in real-time for critical flows
- Set up alerts for unusual error rate spikes
- Track error patterns by flow and action type
- Implement automated recovery for common error types
</Accordion>

<Accordion title="Performance Monitoring">
- Track execution duration trends over time
- Monitor for performance degradation in critical paths
- Set alerts for executions exceeding expected duration
- Use logs to identify bottlenecks in multi-step flows
</Accordion>

## Related Endpoints

<CardGroup cols={2}>
  <Card title="Get Log Entry" icon="file-text" href="/api/executionlogs/get-execution-logs-1">
    Retrieve detailed information for a specific log entry
  </Card>
  <Card title="Flow Executions" icon="workflow" href="/api/flows/get-flows-executions">
    View execution history and status
  </Card>
</CardGroup>

<CardGroup cols={2}>
  <Card title="Export Logs" icon="download" href="/api/executionlogs/post-execution-logs">
    Export logs for external analysis
  </Card>
  <Card title="Execution Metrics" icon="chart-bar" href="/api/flows/get-flows-metrics">
    View aggregated execution metrics
  </Card>
</CardGroup>

---

<Info>
**Pro Tip**: Use the search parameter to quickly find specific error messages or patterns across all your executions. This is especially useful for debugging issues that occur across multiple flows.
</Info>