---
title: "Pagination"
description: "Understanding pagination patterns and efficient data retrieval in the Tolstoy API"
---

# Pagination

The Tolstoy API uses cursor-based pagination for efficient and consistent data retrieval across large datasets. This guide covers pagination patterns, best practices, and optimization strategies.

## Pagination Overview

### **Why Pagination?**

Pagination is essential for:
- **Performance** - Reduces response times and server load
- **Memory Efficiency** - Prevents large data transfers
- **Rate Limit Optimization** - Avoids hitting request limits
- **Consistent Results** - Handles real-time data changes gracefully

### **Pagination Methods**

The Tolstoy API supports multiple pagination approaches:

| Method | Use Case | Pros | Cons |
|--------|----------|------|------|
| **Cursor-based** | Most endpoints | Consistent, handles real-time changes | Complex cursor management |
| **Offset-based** | Simple listing | Easy to implement | Performance degrades with large offsets |
| **Time-based** | Logs, events | Natural ordering | Limited to time-sortable data |

## Cursor-Based Pagination (Recommended)

### **Request Format**

<CodeGroup>

```bash cURL
# First page
curl 'https://api.tolstoy.dev/flows?limit=50' \
  -H 'Authorization: Bearer YOUR_API_KEY'

# Subsequent pages
curl 'https://api.tolstoy.dev/flows?limit=50&cursor=eyJ0aW1lc3RhbXAi...' \
  -H 'Authorization: Bearer YOUR_API_KEY'
```

```javascript JavaScript
// First page
const response = await fetch('https://api.tolstoy.dev/flows?limit=50', {
  headers: {
    'Authorization': `Bearer ${apiKey}`,
    'Content-Type': 'application/json'
  }
});

const data = await response.json();

// Next page using cursor
if (data.pagination.next_cursor) {
  const nextResponse = await fetch(`https://api.tolstoy.dev/flows?limit=50&cursor=${data.pagination.next_cursor}`, {
    headers: {
      'Authorization': `Bearer ${apiKey}`,
      'Content-Type': 'application/json'
    }
  });
}
```

```python Python
import requests

# First page
response = requests.get(
    'https://api.tolstoy.dev/flows',
    headers={'Authorization': f'Bearer {api_key}'},
    params={'limit': 50}
)

data = response.json()

# Next page
if data['pagination']['next_cursor']:
    next_response = requests.get(
        'https://api.tolstoy.dev/flows',
        headers={'Authorization': f'Bearer {api_key}'},
        params={
            'limit': 50,
            'cursor': data['pagination']['next_cursor']
        }
    )
```

</CodeGroup>

### **Response Format**

```json
{
  "flows": [
    {
      "id": "flow_123456789",
      "name": "Customer Onboarding",
      "status": "active",
      "created_at": "2024-01-15T10:30:00Z",
      // ... other flow fields
    }
    // ... more flows
  ],
  "pagination": {
    "limit": 50,
    "count": 50,
    "has_more": true,
    "next_cursor": "eyJ0aW1lc3RhbXAiOiIyMDI0LTAxLTE1VDEwOjMwOjAwWiIsImlkIjoiZmxvd18xMjM0NTY3ODkifQ==",
    "prev_cursor": null,
    "total_estimate": 2500
  }
}
```

### **Pagination Object Fields**

| Field | Type | Description |
|-------|------|-------------|
| `limit` | number | Requested number of items per page |
| `count` | number | Actual number of items returned in this page |
| `has_more` | boolean | Whether more pages are available |
| `next_cursor` | string | Cursor for the next page (null if no more pages) |
| `prev_cursor` | string | Cursor for the previous page (null if first page) |
| `total_estimate` | number | Estimated total number of items (may be approximate) |

## Query Parameters

### **Standard Parameters**

| Parameter | Type | Default | Max | Description |
|-----------|------|---------|-----|-------------|
| `limit` | integer | 20 | 100 | Number of items per page |
| `cursor` | string | null | - | Pagination cursor |
| `sort` | string | `created_at` | - | Sort field |
| `order` | string | `desc` | - | Sort order (`asc` or `desc`) |

### **Filtering Parameters**

Most endpoints support additional filtering:

<CodeGroup>

```bash Filtering Examples
# Filter by status
curl 'https://api.tolstoy.dev/flows?status=active&limit=25' \
  -H 'Authorization: Bearer YOUR_API_KEY'

# Filter by date range
curl 'https://api.tolstoy.dev/execution-logs?start_date=2024-01-01&end_date=2024-01-31&limit=100' \
  -H 'Authorization: Bearer YOUR_API_KEY'

# Search by name
curl 'https://api.tolstoy.dev/actions?search=email&limit=50' \
  -H 'Authorization: Bearer YOUR_API_KEY'

# Multiple filters
curl 'https://api.tolstoy.dev/flows?status=active&created_after=2024-01-01&limit=50' \
  -H 'Authorization: Bearer YOUR_API_KEY'
```

```javascript JavaScript Filtering
const params = new URLSearchParams({
  status: 'active',
  created_after: '2024-01-01',
  limit: '50'
});

const response = await fetch(`https://api.tolstoy.dev/flows?${params}`, {
  headers: {
    'Authorization': `Bearer ${apiKey}`,
    'Content-Type': 'application/json'
  }
});
```

```python Python Filtering
params = {
    'status': 'active',
    'created_after': '2024-01-01',
    'limit': 50
}

response = requests.get(
    'https://api.tolstoy.dev/flows',
    headers={'Authorization': f'Bearer {api_key}'},
    params=params
)
```

</CodeGroup>

## Efficient Pagination Patterns

### **Basic Pagination Iterator**

<CodeGroup>

```javascript JavaScript Iterator
class TolstoyPaginator {
  constructor(apiKey, endpoint, params = {}) {
    this.apiKey = apiKey;
    this.endpoint = endpoint;
    this.params = params;
    this.baseURL = 'https://api.tolstoy.dev';
  }
  
  async *items() {
    let cursor = null;
    
    while (true) {
      const params = new URLSearchParams({
        ...this.params,
        limit: this.params.limit || 100
      });
      
      if (cursor) {
        params.set('cursor', cursor);
      }
      
      const response = await fetch(`${this.baseURL}${this.endpoint}?${params}`, {
        headers: {
          'Authorization': `Bearer ${this.apiKey}`,
          'Content-Type': 'application/json'
        }
      });
      
      if (!response.ok) {
        throw new Error(`HTTP ${response.status}: ${response.statusText}`);
      }
      
      const data = await response.json();
      
      // Yield individual items
      for (const item of data[this.getCollectionKey()]) {
        yield item;
      }
      
      // Check if we should continue
      if (!data.pagination.has_more) {
        break;
      }
      
      cursor = data.pagination.next_cursor;
      
      // Small delay to respect rate limits
      await this.sleep(100);
    }
  }
  
  async *pages() {
    let cursor = null;
    
    while (true) {
      const params = new URLSearchParams({
        ...this.params,
        limit: this.params.limit || 100
      });
      
      if (cursor) {
        params.set('cursor', cursor);
      }
      
      const response = await fetch(`${this.baseURL}${this.endpoint}?${params}`, {
        headers: {
          'Authorization': `Bearer ${this.apiKey}`,
          'Content-Type': 'application/json'
        }
      });
      
      if (!response.ok) {
        throw new Error(`HTTP ${response.status}: ${response.statusText}`);
      }
      
      const data = await response.json();
      yield data;
      
      if (!data.pagination.has_more) {
        break;
      }
      
      cursor = data.pagination.next_cursor;
      await this.sleep(100);
    }
  }
  
  getCollectionKey() {
    // Extract collection key from endpoint
    const match = this.endpoint.match(/\/([^/]+)$/);
    return match ? match[1] : 'items';
  }
  
  sleep(ms) {
    return new Promise(resolve => setTimeout(resolve, ms));
  }
}

// Usage examples

// Iterate through individual items
const flowPaginator = new TolstoyPaginator(apiKey, '/flows', {
  status: 'active',
  limit: 50
});

for await (const flow of flowPaginator.items()) {
  console.log(`Processing flow: ${flow.name}`);
  // Process each flow individually
}

// Iterate through pages
for await (const page of flowPaginator.pages()) {
  console.log(`Page contains ${page.flows.length} flows`);
  console.log(`Estimated total: ${page.pagination.total_estimate}`);
  
  // Process entire page at once
  await processBatch(page.flows);
}
```

```python Python Iterator
import requests
import time
from typing import Iterator, Dict, Any, Optional

class TolstoyPaginator:
    def __init__(self, api_key: str, endpoint: str, params: Dict[str, Any] = None):
        self.api_key = api_key
        self.endpoint = endpoint
        self.params = params or {}
        self.base_url = 'https://api.tolstoy.dev'
        self.session = requests.Session()
        self.session.headers.update({
            'Authorization': f'Bearer {api_key}',
            'Content-Type': 'application/json'
        })
    
    def items(self) -> Iterator[Dict[str, Any]]:
        """Iterate through individual items across all pages"""
        for page in self.pages():
            collection_key = self._get_collection_key()
            for item in page.get(collection_key, []):
                yield item
    
    def pages(self) -> Iterator[Dict[str, Any]]:
        """Iterate through pages"""
        cursor = None
        
        while True:
            # Prepare request parameters
            params = {
                **self.params,
                'limit': self.params.get('limit', 100)
            }
            
            if cursor:
                params['cursor'] = cursor
            
            # Make request
            response = self.session.get(f'{self.base_url}{self.endpoint}', params=params)
            response.raise_for_status()
            
            data = response.json()
            yield data
            
            # Check for more pages
            pagination = data.get('pagination', {})
            if not pagination.get('has_more', False):
                break
            
            cursor = pagination.get('next_cursor')
            
            # Rate limiting delay
            time.sleep(0.1)
    
    def collect_all(self, max_items: Optional[int] = None) -> list:
        """Collect all items into a list (use with caution for large datasets)"""
        items = []
        count = 0
        
        for item in self.items():
            items.append(item)
            count += 1
            
            if max_items and count >= max_items:
                break
        
        return items
    
    def _get_collection_key(self) -> str:
        """Extract collection key from endpoint"""
        parts = self.endpoint.strip('/').split('/')
        return parts[-1] if parts else 'items'

# Usage examples

# Create paginator
paginator = TolstoyPaginator(api_key, '/flows', {
    'status': 'active',
    'limit': 50
})

# Option 1: Process individual items
print("Processing individual flows:")
for flow in paginator.items():
    print(f"- {flow['name']} (ID: {flow['id']})")
    # Process each flow

# Option 2: Process pages
print("\nProcessing page by page:")
for page_data in paginator.pages():
    flows = page_data['flows']
    pagination = page_data['pagination']
    
    print(f"Page has {len(flows)} flows")
    print(f"Has more: {pagination['has_more']}")
    
    # Process entire batch
    for flow in flows:
        # Batch processing logic here
        pass

# Option 3: Collect all (careful with large datasets)
print("\nCollecting all flows:")
all_flows = paginator.collect_all(max_items=1000)  # Limit for safety
print(f"Collected {len(all_flows)} flows total")
```

</CodeGroup>

### **Parallel Pagination**

For better performance, you can paginate multiple endpoints in parallel:

<CodeGroup>

```javascript Parallel Pagination
async function fetchAllData(apiKey) {
  const endpoints = [
    { path: '/flows', key: 'flows' },
    { path: '/actions', key: 'actions' },
    { path: '/tools', key: 'tools' },
    { path: '/execution-logs', key: 'logs' }
  ];
  
  // Start all paginations in parallel
  const promises = endpoints.map(async (endpoint) => {
    const paginator = new TolstoyPaginator(apiKey, endpoint.path);
    const items = [];
    
    for await (const item of paginator.items()) {
      items.push(item);
    }
    
    return { [endpoint.key]: items };
  });
  
  // Wait for all to complete
  const results = await Promise.all(promises);
  
  // Combine results
  return Object.assign({}, ...results);
}

// Usage
const allData = await fetchAllData(apiKey);
console.log(`Loaded ${allData.flows.length} flows`);
console.log(`Loaded ${allData.actions.length} actions`);
console.log(`Loaded ${allData.tools.length} tools`);
console.log(`Loaded ${allData.logs.length} logs`);
```

```python Parallel Pagination
import asyncio
import aiohttp
from typing import Dict, List, Any

class AsyncTolstoyPaginator:
    def __init__(self, api_key: str, endpoint: str, params: Dict[str, Any] = None):
        self.api_key = api_key
        self.endpoint = endpoint
        self.params = params or {}
        self.base_url = 'https://api.tolstoy.dev'
    
    async def collect_all_async(self, session: aiohttp.ClientSession) -> List[Dict[str, Any]]:
        """Collect all items asynchronously"""
        items = []
        cursor = None
        
        while True:
            params = {
                **self.params,
                'limit': self.params.get('limit', 100)
            }
            
            if cursor:
                params['cursor'] = cursor
            
            async with session.get(
                f'{self.base_url}{self.endpoint}',
                headers={'Authorization': f'Bearer {self.api_key}'},
                params=params
            ) as response:
                response.raise_for_status()
                data = await response.json()
                
                collection_key = self._get_collection_key()
                items.extend(data.get(collection_key, []))
                
                pagination = data.get('pagination', {})
                if not pagination.get('has_more', False):
                    break
                
                cursor = pagination.get('next_cursor')
                
                # Small delay for rate limiting
                await asyncio.sleep(0.1)
        
        return items
    
    def _get_collection_key(self) -> str:
        parts = self.endpoint.strip('/').split('/')
        return parts[-1] if parts else 'items'

async def fetch_all_data_parallel(api_key: str) -> Dict[str, List[Any]]:
    """Fetch data from multiple endpoints in parallel"""
    
    endpoints = [
        {'path': '/flows', 'key': 'flows'},
        {'path': '/actions', 'key': 'actions'},
        {'path': '/tools', 'key': 'tools'},
        {'path': '/execution-logs', 'key': 'logs', 'params': {'limit': 100}}
    ]
    
    async with aiohttp.ClientSession() as session:
        # Create paginators
        paginators = [
            AsyncTolstoyPaginator(api_key, ep['path'], ep.get('params', {}))
            for ep in endpoints
        ]
        
        # Fetch all data in parallel
        tasks = [paginator.collect_all_async(session) for paginator in paginators]
        results = await asyncio.gather(*tasks)
        
        # Combine results
        return {
            endpoint['key']: result
            for endpoint, result in zip(endpoints, results)
        }

# Usage
async def main():
    api_key = 'your-api-key'
    all_data = await fetch_all_data_parallel(api_key)
    
    print(f"Loaded {len(all_data['flows'])} flows")
    print(f"Loaded {len(all_data['actions'])} actions")
    print(f"Loaded {len(all_data['tools'])} tools")
    print(f"Loaded {len(all_data['logs'])} logs")

# Run
asyncio.run(main())
```

</CodeGroup>

## Offset-Based Pagination (Legacy)

Some endpoints still support offset-based pagination for backward compatibility:

<CodeGroup>

```bash Offset Pagination
# First page
curl 'https://api.tolstoy.dev/flows?limit=50&offset=0' \
  -H 'Authorization: Bearer YOUR_API_KEY'

# Second page  
curl 'https://api.tolstoy.dev/flows?limit=50&offset=50' \
  -H 'Authorization: Bearer YOUR_API_KEY'
```

```json Offset Response
{
  "flows": [...],
  "pagination": {
    "limit": 50,
    "offset": 0,
    "count": 50,
    "total": 2500,
    "has_more": true,
    "next_offset": 50,
    "prev_offset": null
  }
}
```

</CodeGroup>

<Warning>
**Performance Note:** Offset-based pagination becomes slower with large offsets. Use cursor-based pagination for better performance.
</Warning>

## Time-Based Pagination

For time-series data like execution logs, you can paginate by timestamp:

<CodeGroup>

```bash Time-Based Pagination
# Get logs from the last 24 hours
curl 'https://api.tolstoy.dev/execution-logs?start_time=2024-01-14T10:00:00Z&end_time=2024-01-15T10:00:00Z&limit=100' \
  -H 'Authorization: Bearer YOUR_API_KEY'

# Next batch using last timestamp
curl 'https://api.tolstoy.dev/execution-logs?start_time=2024-01-15T10:00:00Z&limit=100' \
  -H 'Authorization: Bearer YOUR_API_KEY'
```

```javascript Time-Based Iterator
class TimePaginator {
  constructor(apiKey, endpoint, startTime, endTime) {
    this.apiKey = apiKey;
    this.endpoint = endpoint;
    this.startTime = startTime;
    this.endTime = endTime;
    this.currentTime = startTime;
  }
  
  async *items() {
    while (this.currentTime < this.endTime) {
      const params = new URLSearchParams({
        start_time: this.currentTime.toISOString(),
        end_time: this.endTime.toISOString(),
        limit: '1000'
      });
      
      const response = await fetch(`https://api.tolstoy.dev${this.endpoint}?${params}`, {
        headers: {
          'Authorization': `Bearer ${this.apiKey}`,
          'Content-Type': 'application/json'
        }
      });
      
      const data = await response.json();
      const items = data.execution_logs || [];
      
      if (items.length === 0) {
        break;
      }
      
      for (const item of items) {
        yield item;
      }
      
      // Update current time to the last item's timestamp
      const lastItem = items[items.length - 1];
      this.currentTime = new Date(lastItem.created_at);
    }
  }
}

// Usage
const startTime = new Date('2024-01-01T00:00:00Z');
const endTime = new Date('2024-01-31T23:59:59Z');

const timePaginator = new TimePaginator(apiKey, '/execution-logs', startTime, endTime);

for await (const log of timePaginator.items()) {
  console.log(`Log: ${log.id} at ${log.created_at}`);
}
```

</CodeGroup>

## Best Practices

### **Performance Optimization**

<Tabs>
  <Tab title="Page Size Optimization">
    **Choose optimal page sizes:**
    
    ```javascript
    // ‚úÖ Good: Use larger pages for better efficiency
    const paginator = new TolstoyPaginator(apiKey, '/flows', { 
      limit: 100 // Maximum allowed
    });
    
    // ‚ùå Inefficient: Small pages = more requests
    const inefficient = new TolstoyPaginator(apiKey, '/flows', { 
      limit: 10 // Requires 10x more requests
    });
    ```
    
    **Recommendations:**
    - Use maximum page size (100) when possible
    - Smaller pages (20-50) for UI pagination
    - Consider memory constraints for large objects
  </Tab>
  
  <Tab title="Filtering Early">
    **Apply filters to reduce data transfer:**
    
    ```javascript
    // ‚úÖ Good: Filter on server-side
    const activeFlows = new TolstoyPaginator(apiKey, '/flows', {
      status: 'active',
      created_after: '2024-01-01',
      limit: 100
    });
    
    // ‚ùå Inefficient: Filter on client-side  
    const allFlows = new TolstoyPaginator(apiKey, '/flows', { limit: 100 });
    for await (const flow of allFlows.items()) {
      if (flow.status === 'active' && flow.created_at > '2024-01-01') {
        // Process flow
      }
    }
    ```
  </Tab>
  
  <Tab title="Caching Strategies">
    **Cache pages to avoid redundant requests:**
    
    ```javascript
    class CachedPaginator extends TolstoyPaginator {
      constructor(apiKey, endpoint, params = {}) {
        super(apiKey, endpoint, params);
        this.cache = new Map();
      }
      
      async *pages() {
        let cursor = null;
        
        while (true) {
          const cacheKey = `${this.endpoint}:${cursor || 'first'}`;
          
          // Check cache first
          if (this.cache.has(cacheKey)) {
            const cachedPage = this.cache.get(cacheKey);
            yield cachedPage;
            
            if (!cachedPage.pagination.has_more) {
              break;
            }
            cursor = cachedPage.pagination.next_cursor;
            continue;
          }
          
          // Fetch from API
          const page = await this.fetchPage(cursor);
          
          // Cache the page
          this.cache.set(cacheKey, page);
          
          yield page;
          
          if (!page.pagination.has_more) {
            break;
          }
          cursor = page.pagination.next_cursor;
        }
      }
    }
    ```
  </Tab>
  
  <Tab title="Memory Management">
    **Avoid memory leaks with large datasets:**
    
    ```javascript
    // ‚úÖ Good: Process items as you go
    async function processLargeDateset() {
      const paginator = new TolstoyPaginator(apiKey, '/execution-logs');
      
      for await (const log of paginator.items()) {
        await processLog(log);
        // Item is garbage collected after processing
      }
    }
    
    // ‚ùå Memory intensive: Load everything into memory
    async function loadAllData() {
      const paginator = new TolstoyPaginator(apiKey, '/execution-logs');
      const allLogs = [];
      
      for await (const log of paginator.items()) {
        allLogs.push(log); // Accumulates in memory
      }
      
      return allLogs; // Potentially huge array
    }
    ```
  </Tab>
</Tabs>

### **Error Handling in Pagination**

<CodeGroup>

```javascript Robust Pagination
class RobustPaginator extends TolstoyPaginator {
  async *items() {
    let consecutiveErrors = 0;
    const maxErrors = 3;
    
    try {
      for await (const item of super.items()) {
        consecutiveErrors = 0; // Reset on success
        yield item;
      }
    } catch (error) {
      consecutiveErrors++;
      
      console.error(`Pagination error (${consecutiveErrors}/${maxErrors}):`, error.message);
      
      if (consecutiveErrors >= maxErrors) {
        throw new Error(`Pagination failed after ${maxErrors} consecutive errors`);
      }
      
      // Wait and retry
      await this.sleep(1000 * consecutiveErrors);
      yield* this.items(); // Recursive retry
    }
  }
}
```

```python Error-Resilient Pagination
import requests
import time
from typing import Iterator, Dict, Any

class RobustPaginator(TolstoyPaginator):
    def __init__(self, api_key: str, endpoint: str, params: Dict[str, Any] = None, max_retries: int = 3):
        super().__init__(api_key, endpoint, params)
        self.max_retries = max_retries
    
    def pages(self) -> Iterator[Dict[str, Any]]:
        """Paginate with error handling and retries"""
        cursor = None
        consecutive_errors = 0
        
        while True:
            try:
                # Prepare request
                params = {**self.params, 'limit': self.params.get('limit', 100)}
                if cursor:
                    params['cursor'] = cursor
                
                # Make request with timeout
                response = self.session.get(
                    f'{self.base_url}{self.endpoint}',
                    params=params,
                    timeout=30
                )
                response.raise_for_status()
                
                data = response.json()
                consecutive_errors = 0  # Reset on success
                
                yield data
                
                # Check for more pages
                pagination = data.get('pagination', {})
                if not pagination.get('has_more', False):
                    break
                
                cursor = pagination.get('next_cursor')
                time.sleep(0.1)  # Rate limiting
                
            except Exception as e:
                consecutive_errors += 1
                print(f"Pagination error ({consecutive_errors}/{self.max_retries}): {e}")
                
                if consecutive_errors >= self.max_retries:
                    raise Exception(f"Pagination failed after {self.max_retries} consecutive errors") from e
                
                # Exponential backoff
                wait_time = (2 ** consecutive_errors) + 1
                print(f"Retrying in {wait_time} seconds...")
                time.sleep(wait_time)
```

</CodeGroup>

## Pagination Monitoring

Track pagination performance and usage:

<CodeGroup>

```javascript Pagination Metrics
class MetricsPaginator extends TolstoyPaginator {
  constructor(apiKey, endpoint, params = {}) {
    super(apiKey, endpoint, params);
    this.metrics = {
      pagesLoaded: 0,
      itemsProcessed: 0,
      totalTime: 0,
      errors: 0,
      startTime: Date.now()
    };
  }
  
  async *items() {
    const startTime = Date.now();
    
    try {
      for await (const item of super.items()) {
        this.metrics.itemsProcessed++;
        yield item;
      }
    } catch (error) {
      this.metrics.errors++;
      throw error;
    } finally {
      this.metrics.totalTime = Date.now() - startTime;
      this.logMetrics();
    }
  }
  
  async *pages() {
    for await (const page of super.pages()) {
      this.metrics.pagesLoaded++;
      yield page;
    }
  }
  
  logMetrics() {
    const stats = {
      ...this.metrics,
      itemsPerSecond: Math.round(this.metrics.itemsProcessed / (this.metrics.totalTime / 1000)),
      avgTimePerPage: Math.round(this.metrics.totalTime / this.metrics.pagesLoaded)
    };
    
    console.log('üìä Pagination Stats:', stats);
  }
}
```

</CodeGroup>

## Common Issues and Solutions

<AccordionGroup>
  <Accordion title="Cursor Invalidation">
    **Problem:** Cursor becomes invalid due to data changes
    
    **Symptoms:** 
    ```json
    {
      "error": {
        "code": "INVALID_CURSOR",
        "message": "The provided cursor is invalid or expired"
      }
    }
    ```
    
    **Solutions:**
    - Start over from the beginning
    - Implement cursor refresh logic
    - Use time-based pagination for volatile data
    
    ```javascript
    async function paginateWithCursorRecovery() {
      let attempts = 0;
      const maxAttempts = 2;
      
      while (attempts < maxAttempts) {
        try {
          const paginator = new TolstoyPaginator(apiKey, '/flows');
          for await (const item of paginator.items()) {
            yield item;
          }
          break; // Success
          
        } catch (error) {
          if (error.code === 'INVALID_CURSOR' && attempts < maxAttempts - 1) {
            console.log('Cursor invalid, restarting pagination...');
            attempts++;
            continue;
          }
          throw error;
        }
      }
    }
    ```
  </Accordion>
  
  <Accordion title="Inconsistent Results">
    **Problem:** Items appear multiple times or are missed during pagination
    
    **Cause:** Data changes during pagination affect ordering
    
    **Solutions:**
    - Use stable sort fields (ID, created_at)
    - Implement deduplication
    - Use consistent time ranges
    
    ```javascript
    class DeduplicatedPaginator extends TolstoyPaginator {
      constructor(apiKey, endpoint, params = {}) {
        super(apiKey, endpoint, params);
        this.seenIds = new Set();
      }
      
      async *items() {
        for await (const item of super.items()) {
          if (!this.seenIds.has(item.id)) {
            this.seenIds.add(item.id);
            yield item;
          }
        }
      }
    }
    ```
  </Accordion>
  
  <Accordion title="Memory Issues">
    **Problem:** Large datasets cause memory exhaustion
    
    **Solutions:**
    - Process items individually instead of collecting all
    - Use streaming for large datasets
    - Implement backpressure control
    
    ```javascript
    // ‚úÖ Memory-efficient streaming
    async function streamProcess() {
      const paginator = new TolstoyPaginator(apiKey, '/execution-logs');
      
      for await (const log of paginator.items()) {
        await processLogItem(log);
        // Memory is freed after each item
      }
    }
    ```
  </Accordion>
</AccordionGroup>

## Next Steps

<CardGroup cols={2}>
  <Card title="Rate Limiting" icon="gauge" href="/api/rate-limiting">
    Optimize pagination for rate limits
  </Card>
  <Card title="Error Handling" icon="triangle-exclamation" href="/api/errors">
    Handle pagination errors effectively
  </Card>
  <Card title="Organizations API" icon="building" href="/api/organizations/list">
    Start with organization endpoints
  </Card>
  <Card title="Best Practices" icon="star" href="/best-practices">
    API optimization techniques
  </Card>
</CardGroup>